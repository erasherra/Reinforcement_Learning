{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TX00DQ05-3001 Exercises 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import numpy.linalg as LA\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 1: Sample behaviour of an MDP\n",
    "\n",
    "Let's take (again) a look at Sutton & Barto example 4.1 gridworld. On each iteration start at every (non-terminating) state and sample actions in succeeding states by selecting them from uniform distribution (each action - up, down, left, right - is equally probable). Run the episode until terminal state is encountered. Collect statistics to calculate average number of steps needed before completion for each start state. Should this number match with something you have seen earlier in the exercises?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(6, 4), (6, 6), (6, 3), (7, 4)]\n",
      "[[0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "60\n",
      "[[117.2  110.14 111.42 105.89 105.71 107.31 107.04 103.17]\n",
      " [112.95 112.75 105.8  103.39 103.16 102.22  96.86  99.77]\n",
      " [103.25 101.15 100.93  94.58  94.44  95.74  91.95  93.17]\n",
      " [ 97.74  89.62  85.72  84.57  80.78  75.93  80.05  78.72]\n",
      " [ 88.89  83.31  67.21  64.95  60.22  57.86  59.6   56.39]\n",
      " [ 74.65  69.64  52.37  43.39  37.38  36.21  36.99  38.85]\n",
      " [ 66.73  55.13  39.1    0.     0.    10.48   0.    18.89]\n",
      " [ 57.61  55.33  36.7   12.29   0.     9.88   9.29  16.74]]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def calculateValueForCurrentState(value,stepcost):\n",
    "    #print(\"(\",stepcost,\"+\",value,\")*0.25 = \",((stepcost+(value))*0.25))\n",
    "    return ((stepcost+(value))*0.25)\n",
    "\n",
    "\n",
    "def checkWithBoundary(max,r,c,oR,oC):\n",
    "    #print(\"-nstate: \",r,\",\",c)\n",
    "    #print(V)\n",
    "    if 0 > r or max <= r or 0 > c or max <= c :\n",
    "        #print(oR,\",\",oC,\" value:\",V[oR,oC])\n",
    "        return (oR,oC)\n",
    "    else:\n",
    "        #print(r,\",\",c,\" value:\",V[r,c])\n",
    "        return (r,c)\n",
    "\n",
    "    \n",
    "    \n",
    "def Actions(val):\n",
    "    if val == 0:\n",
    "        return [1,0]\n",
    "    elif val == 1:\n",
    "        return [-1,0]\n",
    "    elif val == 2:\n",
    "        return [0,1]\n",
    "    elif val == 3:\n",
    "        return [0,-1]\n",
    "    \n",
    "\n",
    "def randomAction():\n",
    "    return Actions(random.randint(0,3))\n",
    "\n",
    "def makeAction(maxsize,r,c):\n",
    "    a = randomAction()\n",
    "    \n",
    "    \n",
    "    return checkWithBoundary(maxsize,r+a[0],c+a[1],r,c)\n",
    "\n",
    "def checkIfInTerminatingPoint(pos,term):\n",
    "    for t in term:\n",
    "        if pos == t:\n",
    "            return True\n",
    "    return False\n",
    "    \n",
    "class Agent(object):\n",
    "    steps = 0\n",
    "    def __init__(self, startPos):\n",
    "        self.startPos = startPos\n",
    "        self.pos = startPos\n",
    "    \n",
    "def MC_pathlengths(maxiters,gridworld,terminating):\n",
    "    \n",
    "    lengths= gridworld\n",
    "    agents =[]\n",
    "    \n",
    "    for x in range(lengths.shape[0]):\n",
    "        for y in range(lengths.shape[1]):\n",
    "            if not checkIfInTerminatingPoint((x,y),terminating):\n",
    "                agents.append( Agent((x,y)) )\n",
    "    print(len(agents))\n",
    "    for i in range(1, maxiters):\n",
    "\n",
    "        goals = []\n",
    "        calc = 0\n",
    "        \n",
    "        while (len(agents)>0):\n",
    "            for agent in agents:\n",
    "                agent.pos = makeAction(lengths.shape[0],agent.pos[0],agent.pos[1])\n",
    "                agent.steps += 1\n",
    "                if (checkIfInTerminatingPoint(agent.pos,terminating)):\n",
    "                    agents.remove(agent)\n",
    "                    goals.append(agent)\n",
    "                    break\n",
    "                    \n",
    "            \n",
    "        for agent in goals:\n",
    "            mean = ((i-1)/i) * lengths[agent.startPos] + agent.steps/i\n",
    "            #print(((i-1)/i),\"*\",lengths[agent.startPos],\"+\",agent.steps/i,\"=\",mean)\n",
    "            lengths[agent.startPos] = mean\n",
    "            agent.steps = 0\n",
    "            agent.pos = agent.startPos\n",
    "            \n",
    "        agents = goals\n",
    "    return lengths       \n",
    "    \n",
    "    \n",
    "def CreateGridWorld(gridsize,terminating = None,numberOfTerminating = None):\n",
    "    \n",
    "\n",
    "    if(terminating == None):\n",
    "        if(numberOfTerminating == 0 or numberOfTerminating == None):\n",
    "            numberOfTerminating = 1\n",
    "        terminating = []\n",
    "        for n in range(numberOfTerminating):\n",
    "            terminating.append((random.randint(0,gridsize[0]-1), \\\n",
    "                        random.randint(0,gridsize[1]-1)))\n",
    "        \n",
    "        \n",
    "    gridworld = np.zeros((gridsize[0], gridsize[1]))\n",
    "    \n",
    "    print(terminating)\n",
    "    print(gridworld)\n",
    "    return gridworld, terminating\n",
    "\n",
    "\n",
    "gridworld,terminating = CreateGridWorld((8,8),None,4)\n",
    "with np.printoptions(precision=2):\n",
    "    print(MC_pathlengths(1000,gridworld,terminating))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.         14.66666667 32.11111111 31.66666667]\n",
      " [10.22222222  9.44444444 16.88888889 14.22222222]\n",
      " [11.22222222 35.77777778 20.         24.55555556]\n",
      " [29.44444444 22.33333333 28.88888889  0.        ]]\n"
     ]
    }
   ],
   "source": [
    "gridSize = (4,4)\n",
    "actions = [\n",
    "    (0,-1), #up\n",
    "    (0, 1), #down\n",
    "    (-1, 0),#left\n",
    "    (1, 0)  #right\n",
    "]\n",
    "terminationPoints = [(0,0), (3,3)]\n",
    "\n",
    "def terminationPoint(pos):\n",
    "    for t in terminationPoints:\n",
    "        if pos == t:\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "def doAction(pos):\n",
    "    a = actions[random.randint(0,3)]\n",
    "    newPos = tuple(map(lambda x, y: x + y, pos, a))\n",
    "    # Check boundaries\n",
    "    if (newPos[0] >= gridSize[0]):\n",
    "        newPos = (gridSize[0]-1, pos[1])\n",
    "    if (newPos[0] < 0):\n",
    "        newPos = (0, pos[1])\n",
    "    if (newPos[1] >= gridSize[1]):\n",
    "        newPos = (pos[0], gridSize[1]-1)\n",
    "    if (newPos[1] < 0):\n",
    "        newPos = (pos[0], 0)\n",
    "    \n",
    "    return newPos\n",
    "\n",
    "class Agent(object):\n",
    "    steps = 0\n",
    "    def __init__(self, startPos):\n",
    "        self.startPos = startPos\n",
    "        self.pos = startPos\n",
    "\n",
    "def MC_pathlengths(maxiters):\n",
    "    lengths = np.zeros(gridSize)\n",
    "    \n",
    "    # Initialize agents\n",
    "    agentList = []\n",
    "    aIndex = 0\n",
    "    for x in range(gridSize[0]):\n",
    "        for y in range(gridSize[1]):\n",
    "            if not terminationPoint((x,y)):\n",
    "                agentList.append( Agent((x,y)) )\n",
    "            aIndex += 1\n",
    "    for i in range(1, maxiters):\n",
    "        # List for agents who have reached termination point\n",
    "        stoppedAgentList = []\n",
    "        \n",
    "        # Loop while there is still agents who have not\n",
    "        # reached term point\n",
    "        while (len(agentList) > 0):\n",
    "            for agent in agentList:\n",
    "                agent.pos = doAction(agent.pos)\n",
    "                agent.steps += 1\n",
    "                if (terminationPoint(agent.pos)):\n",
    "                    agentList.remove(agent)\n",
    "                    stoppedAgentList.append(agent)\n",
    "        \n",
    "        # Calculate new mean and reset agents\n",
    "        for agent in stoppedAgentList:\n",
    "            mean = ((i-1)/i) * lengths[agent.startPos] + agent.steps/i\n",
    "            #print(((i-1)/i),\"*\",lengths[agent.startPos],\"+\",agent.steps/i,\"=\",mean)\n",
    "            lengths[agent.startPos] = mean\n",
    "            agent.steps = 0\n",
    "            agent.pos = agent.startPos\n",
    "            \n",
    "        #Put back for next iteration\n",
    "        agentList = stoppedAgentList\n",
    "            \n",
    "    return lengths\n",
    "\n",
    "print(MC_pathlengths(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE\n",
    "\n",
    "def MC_pathlengths(maxiters):\n",
    "    lengths = dict()\n",
    "    # YOUR CODE\n",
    "\n",
    "    return lengths\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2: Monte Carlo state value function estimation. \n",
    "\n",
    "Calculate state-value function V for the gridworld of Sutton & Barto example 4.1 using first-visit or every-visit Monte Carlo policy evaluation (see for example page 92 of Sutton & Barto). Policy to be evaluated is the same as before; each action (up, down, left, right) is equally probable.  Action that would result in leaving the grid (for example moving up in top row) will leave state unchanged (but action has been taken). Gamma (discount factor) is assumed to be = 1, ie. no discounting.\n",
    "\n",
    "Try out both exploring starts (see Sutton & Barto, p. 96) and fixed start points. Any difference?\n",
    "\n",
    "Take a look at the value function you get when you run the algorithm multiple times (with fixed # of iterations). Any observations?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  0.   -14.89 -20.72 -22.48]\n",
      " [-14.33 -18.32 -20.87 -21.31]\n",
      " [-20.25 -20.02 -18.3  -14.76]\n",
      " [-23.01 -20.63 -14.43   0.  ]]\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 3*: Monte Carlo action value function estimation\n",
    "\n",
    "Use the same idea as in exercise 2 to estimate q function.\n",
    "\n",
    "*) - not mandatory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 4*: Monte Carlo control\n",
    "\n",
    "Compute the optimal policy for the 4x4 gridworld example. Start with random policy. Consider the epsilon adjustment schedule - can it in practise be 1/k, or is something more conservative better? Can you think of any other tricks to manage the noisiness of MC?\n",
    "\n",
    "*) - not mandatory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
