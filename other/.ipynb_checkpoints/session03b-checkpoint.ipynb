{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TX00DQ05-3001 Exercises 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import numpy.linalg as LA\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 1: Sample behaviour of an MDP\n",
    "\n",
    "Let's take (again) a look at Sutton & Barto example 4.1 gridworld. On each iteration start at every (non-terminating) state and sample actions in succeeding states by selecting them from uniform distribution (each action - up, down, left, right - is equally probable). Run the episode until terminal state is encountered. Collect statistics to calculate average number of steps needed before completion for each start state. Should this number match with something you have seen earlier in the exercises?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.          6.15827338  6.14705882  9.01020408]\n",
      " [ 7.37815126  6.43181818  6.34814815  7.12195122]\n",
      " [10.2         5.26582278  8.30188679  7.        ]\n",
      " [ 4.58100559  8.37735849  4.57541899  0.        ]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[ 0.        ,  6.15827338,  6.14705882,  9.01020408],\n",
       "       [ 7.37815126,  6.43181818,  6.34814815,  7.12195122],\n",
       "       [10.2       ,  5.26582278,  8.30188679,  7.        ],\n",
       "       [ 4.58100559,  8.37735849,  4.57541899,  0.        ]])"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "STATES = []\n",
    "\n",
    "def calculateValueForCurrentState(value,stepcost):\n",
    "    #print(\"(\",stepcost,\"+\",value,\")*0.25 = \",((stepcost+(value))*0.25))\n",
    "    return ((stepcost+(value))*0.25)\n",
    "\n",
    "\n",
    "def checkWithBoundary(r,c,oR,oC):\n",
    "    #print(\"-nstate: \",r,\",\",c)\n",
    "    #print(V)\n",
    "    if 0 > r or 4 <= r or 0 > c or 4 <= c :\n",
    "        #print(oR,\",\",oC,\" value:\",V[oR,oC])\n",
    "        return [oR,oC]\n",
    "    else:\n",
    "        #print(r,\",\",c,\" value:\",V[r,c])\n",
    "        return [r,c]\n",
    "\n",
    "    \n",
    "    \n",
    "def Action_Policy(val):\n",
    "    if val == 0:\n",
    "        return [1,0]\n",
    "    elif val == 1:\n",
    "        return [-1,0]\n",
    "    elif val == 2:\n",
    "        return [0,1]\n",
    "    elif val == 3:\n",
    "        return [0,-1]\n",
    "    \n",
    "\n",
    "def randomAction():\n",
    "    return Action_Policy(random.randint(0,3))\n",
    "\n",
    "def makeAction(v,r,c):\n",
    "    a = randomAction()\n",
    "    \n",
    "    \n",
    "    return checkWithBoundary(r+a[0],c+a[1],r,c)\n",
    "\n",
    "def checkStates(r,c):\n",
    "    for s in STATES:\n",
    "        if (r,c) in s:\n",
    "            return True\n",
    "    STATES.append([r,c])    \n",
    "    return False\n",
    "    \n",
    "    \n",
    "def MC_pathlengths(maxiters,term,v,r,c):\n",
    "    lengths = dict()\n",
    "    # YOUR CODE\n",
    "    leng = []\n",
    "    l=0\n",
    "    row = r\n",
    "    col = c\n",
    "    new_state = [0,0]\n",
    "    for i in range(maxiters):\n",
    "        new_state = makeAction(v,row,col)\n",
    "        row = new_state[0]\n",
    "        col = new_state[1]\n",
    "        if checkStates(row,col) == False:\n",
    "            if (row,col) in term:\n",
    "                leng.append(l)\n",
    "            \n",
    "                l = 0\n",
    "            else:\n",
    "                l+=1\n",
    "            \n",
    "    #print(leng)\n",
    "    return leng\n",
    "\n",
    "def calculatePathMean(lengths):\n",
    "    return np.mean(lengths)\n",
    "        \n",
    "    \n",
    "    \n",
    "def IterateValue(gridworld,terminating,stepcost,eps,maxiters):\n",
    "    \n",
    "    V = gridworld\n",
    "    V_new = np.zeros((V.shape[0], V.shape[1]))\n",
    "    #for i in range(maxiters):\n",
    "    # YOUR CODE HERE\n",
    "    #print(V)\n",
    "    lenght =0\n",
    "    for c in range(V.shape[0]):\n",
    "        for r in range(V.shape[1]):\n",
    "            #print(\"in (\",r,\",\",c,\")\")\n",
    "            if not (r, c) in terminating:\n",
    "                lenght=calculatePathMean(MC_pathlengths(maxiters,terminating,V,r,c))\n",
    "                #V_new[r,c] = calculateValueForCurrentState(lenght,stepcost)\n",
    "                V_new[r,c] = lenght\n",
    "        \n",
    "    print(V_new)\n",
    "    return V_new\n",
    "rows_count = 4\n",
    "columns_count = 4\n",
    "V = np.zeros((rows_count, columns_count))\n",
    "V_new = np.zeros((rows_count, columns_count))\n",
    "terminating = [(0,0), (rows_count-1, columns_count-1)]\n",
    "stepcost = -1\n",
    "\n",
    "\n",
    "\n",
    "eps = 0.0000001\n",
    "maxiters = 1000\n",
    "IterateValue(V_new,terminating,stepcost,eps,maxiters)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "14\n",
      "0\n",
      "14\n",
      "0\n",
      "14\n",
      "0\n",
      "14\n",
      "0\n",
      "14\n",
      "0\n",
      "14\n",
      "0\n",
      "14\n",
      "0\n",
      "14\n",
      "0\n",
      "14\n",
      "[[ 0.         13.44444444 23.         25.22222222]\n",
      " [ 6.44444444 24.         15.11111111 21.44444444]\n",
      " [25.77777778 17.66666667 23.77777778  9.11111111]\n",
      " [16.         24.44444444 15.44444444  0.        ]]\n"
     ]
    }
   ],
   "source": [
    "gridSize = (4,4)\n",
    "actions = [\n",
    "    (0,-1), #up\n",
    "    (0, 1), #down\n",
    "    (-1, 0),#left\n",
    "    (1, 0)  #right\n",
    "]\n",
    "terminationPoints = [(0,0), (3,3)]\n",
    "\n",
    "def terminationPoint(pos):\n",
    "    for t in terminationPoints:\n",
    "        if pos == t:\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "def doAction(pos):\n",
    "    a = actions[random.randint(0,3)]\n",
    "    newPos = tuple(map(lambda x, y: x + y, pos, a))\n",
    "    # Check boundaries\n",
    "    if (newPos[0] >= gridSize[0]):\n",
    "        newPos = (gridSize[0]-1, pos[1])\n",
    "    if (newPos[0] < 0):\n",
    "        newPos = (0, pos[1])\n",
    "    if (newPos[1] >= gridSize[1]):\n",
    "        newPos = (pos[0], gridSize[1]-1)\n",
    "    if (newPos[1] < 0):\n",
    "        newPos = (pos[0], 0)\n",
    "    \n",
    "    return newPos\n",
    "\n",
    "class Agent(object):\n",
    "    steps = 0\n",
    "    def __init__(self, startPos):\n",
    "        self.startPos = startPos\n",
    "        self.pos = startPos\n",
    "\n",
    "def MC_pathlengths(maxiters):\n",
    "    lengths = np.zeros(gridSize)\n",
    "    \n",
    "    # Initialize agents\n",
    "    agentList = []\n",
    "    aIndex = 0\n",
    "    for x in range(gridSize[0]):\n",
    "        for y in range(gridSize[1]):\n",
    "            if not terminationPoint((x,y)):\n",
    "                agentList.append( Agent((x,y)) )\n",
    "            aIndex += 1\n",
    "    for i in range(1, maxiters):\n",
    "        # List for agents who have reached termination point\n",
    "        stoppedAgentList = []\n",
    "        \n",
    "        # Loop while there is still agents who have not\n",
    "        # reached term point\n",
    "        while (len(agentList) > 0):\n",
    "            for agent in agentList:\n",
    "                agent.pos = doAction(agent.pos)\n",
    "                agent.steps += 1\n",
    "                if (terminationPoint(agent.pos)):\n",
    "                    agentList.remove(agent)\n",
    "                    stoppedAgentList.append(agent)\n",
    "        \n",
    "        # Calculate new mean and reset agents\n",
    "        for agent in stoppedAgentList:\n",
    "            mean = ((i-1)/i) * lengths[agent.startPos] + agent.steps/i\n",
    "            print(((i-1)/i),\"*\",lengths[agent.startPos],\"+\",agent.steps/i,\"=\",mean)\n",
    "            lengths[agent.startPos] = mean\n",
    "            agent.steps = 0\n",
    "            agent.pos = agent.startPos\n",
    "            \n",
    "        #Put back for next iteration\n",
    "        agentList = stoppedAgentList\n",
    "            \n",
    "    return lengths\n",
    "\n",
    "print(MC_pathlengths(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE\n",
    "\n",
    "def MC_pathlengths(maxiters):\n",
    "    lengths = dict()\n",
    "    # YOUR CODE\n",
    "\n",
    "    return lengths\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2: Monte Carlo state value function estimation. \n",
    "\n",
    "Calculate state-value function V for the gridworld of Sutton & Barto example 4.1 using first-visit or every-visit Monte Carlo policy evaluation (see for example page 92 of Sutton & Barto). Policy to be evaluated is the same as before; each action (up, down, left, right) is equally probable.  Action that would result in leaving the grid (for example moving up in top row) will leave state unchanged (but action has been taken). Gamma (discount factor) is assumed to be = 1, ie. no discounting.\n",
    "\n",
    "Try out both exploring starts (see Sutton & Barto, p. 96) and fixed start points. Any difference?\n",
    "\n",
    "Take a look at the value function you get when you run the algorithm multiple times (with fixed # of iterations). Any observations?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  0.   -14.89 -20.72 -22.48]\n",
      " [-14.33 -18.32 -20.87 -21.31]\n",
      " [-20.25 -20.02 -18.3  -14.76]\n",
      " [-23.01 -20.63 -14.43   0.  ]]\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 3*: Monte Carlo action value function estimation\n",
    "\n",
    "Use the same idea as in exercise 2 to estimate q function.\n",
    "\n",
    "*) - not mandatory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 4*: Monte Carlo control\n",
    "\n",
    "Compute the optimal policy for the 4x4 gridworld example. Start with random policy. Consider the epsilon adjustment schedule - can it in practise be 1/k, or is something more conservative better? Can you think of any other tricks to manage the noisiness of MC?\n",
    "\n",
    "*) - not mandatory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "GAMMA = 0.9\n",
    "EPSILON=0.2\n",
    "ALL_POSSIBLE_ACTIONS = ('U', 'D', 'L', 'R')\n",
    "N_EPISODES = 10000\n",
    "\n",
    "def epsilon_action(a, eps=0.1):\n",
    "    p = np.random.random()\n",
    "    if p < (1 - eps):\n",
    "        return a\n",
    "    else:\n",
    "        return np.random.choice(ALL_POSSIBLE_ACTIONS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
