{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TX00DQ05-3001 Exercises 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import numpy.linalg as LA\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 1: Sample behaviour of an MDP\n",
    "\n",
    "Let's take (again) a look at Sutton & Barto example 4.1 gridworld. On each iteration start at every (non-terminating) state and sample actions in succeeding states by selecting them from uniform distribution (each action - up, down, left, right - is equally probable). Run the episode until terminal state is encountered. Collect statistics to calculate average number of steps needed before completion for each start state. Should this number match with something you have seen earlier in the exercises?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(4, 6), (4, 1), (4, 3), (3, 5)]\n",
      "[[0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "60\n",
      "[[42.81881882 44.31531532 40.5975976  40.30930931 39.15815816 39.67967968\n",
      "  36.46846847 38.21921922]\n",
      " [39.34434434 40.47247247 40.8008008  36.83483483 34.62862863 32.28828829\n",
      "  34.05505506 34.11911912]\n",
      " [33.93393393 33.07107107 30.54154154 27.13513514 26.24724725 22.28728729\n",
      "  24.52152152 27.18718719]\n",
      " [23.42842843 21.1961962  21.5975976  16.76176176 13.65265265  0.\n",
      "  12.1991992  20.01901902]\n",
      " [17.98098098  0.         12.83883884  0.         12.6006006   8.47947948\n",
      "   0.         16.20720721]\n",
      " [25.11311311 19.48748749 19.78178178 17.5955956  20.3973974  21.73373373\n",
      "  17.67267267 22.87987988]\n",
      " [29.08008008 26.03903904 27.68768769 26.99399399 26.82182182 26.68468468\n",
      "  29.02802803 30.3013013 ]\n",
      " [34.05405405 31.87787788 31.55855856 30.39439439 31.92592593 31.47247247\n",
      "  33.33733734 34.09209209]]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def calculateValueForCurrentState(value,stepcost):\n",
    "    #print(\"(\",stepcost,\"+\",value,\")*0.25 = \",((stepcost+(value))*0.25))\n",
    "    return ((stepcost+(value))*0.25)\n",
    "\n",
    "\n",
    "def checkWithBoundary(max,r,c,oR,oC):\n",
    "    #print(\"-nstate: \",r,\",\",c)\n",
    "    #print(V)\n",
    "    if 0 > r or max <= r or 0 > c or max <= c :\n",
    "        #print(oR,\",\",oC,\" value:\",V[oR,oC])\n",
    "        return (oR,oC)\n",
    "    else:\n",
    "        #print(r,\",\",c,\" value:\",V[r,c])\n",
    "        return (r,c)\n",
    "\n",
    "    \n",
    "    \n",
    "def Actions(val):\n",
    "    if val == 0:\n",
    "        return [1,0]\n",
    "    elif val == 1:\n",
    "        return [-1,0]\n",
    "    elif val == 2:\n",
    "        return [0,1]\n",
    "    elif val == 3:\n",
    "        return [0,-1]\n",
    "    \n",
    "\n",
    "def randomAction():\n",
    "    return Actions(random.randint(0,3))\n",
    "\n",
    "def makeAction(maxsize,r,c):\n",
    "    a = randomAction()\n",
    "    \n",
    "    \n",
    "    return checkWithBoundary(maxsize,r+a[0],c+a[1],r,c)\n",
    "\n",
    "def checkIfInTerminatingPoint(pos,term):\n",
    "    for t in term:\n",
    "        if pos == t:\n",
    "            return True\n",
    "    return False\n",
    "    \n",
    "class Agent(object):\n",
    "    steps = 0\n",
    "    def __init__(self, startPos):\n",
    "        self.startPos = startPos\n",
    "        self.pos = startPos\n",
    "    \n",
    "def MC_pathlengths(maxiters,gridworld,terminating):\n",
    "    \n",
    "    lengths= gridworld\n",
    "    agents =[]\n",
    "    \n",
    "    for x in range(lengths.shape[0]):\n",
    "        for y in range(lengths.shape[1]):\n",
    "            if not checkIfInTerminatingPoint((x,y),terminating):\n",
    "                agents.append( Agent((x,y)) )\n",
    "    print(len(agents))\n",
    "    for i in range(1, maxiters):\n",
    "\n",
    "        goals = []\n",
    "        calc = 0\n",
    "        \n",
    "        while (len(agents)>0):\n",
    "            for agent in agents:\n",
    "                agent.pos = makeAction(lengths.shape[0],agent.pos[0],agent.pos[1])\n",
    "                agent.steps += 1\n",
    "                if (checkIfInTerminatingPoint(agent.pos,terminating)):\n",
    "                    agents.remove(agent)\n",
    "                    goals.append(agent)\n",
    "                    break\n",
    "                    \n",
    "            \n",
    "        for agent in goals:\n",
    "            mean = ((i-1)/i) * lengths[agent.startPos] + agent.steps/i\n",
    "            #print(((i-1)/i),\"*\",lengths[agent.startPos],\"+\",agent.steps/i,\"=\",mean)\n",
    "            lengths[agent.startPos] = mean\n",
    "            agent.steps = 0\n",
    "            agent.pos = agent.startPos\n",
    "            \n",
    "        agents = goals\n",
    "    return lengths       \n",
    "    \n",
    "    \n",
    "def CreateGridWorld(gridsize,terminating = None,numberOfTerminating = None):\n",
    "    \n",
    "\n",
    "    if(terminating == None):\n",
    "        if(numberOfTerminating == 0 or numberOfTerminating == None):\n",
    "            numberOfTerminating = 1\n",
    "        terminating = []\n",
    "        for n in range(numberOfTerminating):\n",
    "            terminating.append((random.randint(0,gridsize[0]-1), \\\n",
    "                        random.randint(0,gridsize[1]-1)))\n",
    "        \n",
    "        \n",
    "    gridworld = np.zeros((gridsize[0], gridsize[1]))\n",
    "    \n",
    "    print(terminating)\n",
    "    print(gridworld)\n",
    "    return gridworld, terminating\n",
    "\n",
    "\n",
    "gridworld,terminating = CreateGridWorld((8,8),None,4)\n",
    "with np.printoptions(precision=2):\n",
    "    print(MC_pathlengths(1000,gridworld,terminating))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.         14.66666667 32.11111111 31.66666667]\n",
      " [10.22222222  9.44444444 16.88888889 14.22222222]\n",
      " [11.22222222 35.77777778 20.         24.55555556]\n",
      " [29.44444444 22.33333333 28.88888889  0.        ]]\n"
     ]
    }
   ],
   "source": [
    "gridSize = (4,4)\n",
    "actions = [\n",
    "    (0,-1), #up\n",
    "    (0, 1), #down\n",
    "    (-1, 0),#left\n",
    "    (1, 0)  #right\n",
    "]\n",
    "terminationPoints = [(0,0), (3,3)]\n",
    "\n",
    "def terminationPoint(pos):\n",
    "    for t in terminationPoints:\n",
    "        if pos == t:\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "def doAction(pos):\n",
    "    a = actions[random.randint(0,3)]\n",
    "    newPos = tuple(map(lambda x, y: x + y, pos, a))\n",
    "    # Check boundaries\n",
    "    if (newPos[0] >= gridSize[0]):\n",
    "        newPos = (gridSize[0]-1, pos[1])\n",
    "    if (newPos[0] < 0):\n",
    "        newPos = (0, pos[1])\n",
    "    if (newPos[1] >= gridSize[1]):\n",
    "        newPos = (pos[0], gridSize[1]-1)\n",
    "    if (newPos[1] < 0):\n",
    "        newPos = (pos[0], 0)\n",
    "    \n",
    "    return newPos\n",
    "\n",
    "class Agent(object):\n",
    "    steps = 0\n",
    "    def __init__(self, startPos):\n",
    "        self.startPos = startPos\n",
    "        self.pos = startPos\n",
    "\n",
    "def MC_pathlengths(maxiters):\n",
    "    lengths = np.zeros(gridSize)\n",
    "    \n",
    "    # Initialize agents\n",
    "    agentList = []\n",
    "    aIndex = 0\n",
    "    for x in range(gridSize[0]):\n",
    "        for y in range(gridSize[1]):\n",
    "            if not terminationPoint((x,y)):\n",
    "                agentList.append( Agent((x,y)) )\n",
    "            aIndex += 1\n",
    "    for i in range(1, maxiters):\n",
    "        # List for agents who have reached termination point\n",
    "        stoppedAgentList = []\n",
    "        \n",
    "        # Loop while there is still agents who have not\n",
    "        # reached term point\n",
    "        while (len(agentList) > 0):\n",
    "            for agent in agentList:\n",
    "                agent.pos = doAction(agent.pos)\n",
    "                agent.steps += 1\n",
    "                if (terminationPoint(agent.pos)):\n",
    "                    agentList.remove(agent)\n",
    "                    stoppedAgentList.append(agent)\n",
    "        \n",
    "        # Calculate new mean and reset agents\n",
    "        for agent in stoppedAgentList:\n",
    "            mean = ((i-1)/i) * lengths[agent.startPos] + agent.steps/i\n",
    "            #print(((i-1)/i),\"*\",lengths[agent.startPos],\"+\",agent.steps/i,\"=\",mean)\n",
    "            lengths[agent.startPos] = mean\n",
    "            agent.steps = 0\n",
    "            agent.pos = agent.startPos\n",
    "            \n",
    "        #Put back for next iteration\n",
    "        agentList = stoppedAgentList\n",
    "            \n",
    "    return lengths\n",
    "\n",
    "print(MC_pathlengths(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE\n",
    "\n",
    "def MC_pathlengths(maxiters):\n",
    "    lengths = dict()\n",
    "    # YOUR CODE\n",
    "\n",
    "    return lengths\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2: Monte Carlo state value function estimation. \n",
    "\n",
    "Calculate state-value function V for the gridworld of Sutton & Barto example 4.1 using first-visit or every-visit Monte Carlo policy evaluation (see for example page 92 of Sutton & Barto). Policy to be evaluated is the same as before; each action (up, down, left, right) is equally probable.  Action that would result in leaving the grid (for example moving up in top row) will leave state unchanged (but action has been taken). Gamma (discount factor) is assumed to be = 1, ie. no discounting.\n",
    "\n",
    "Try out both exploring starts (see Sutton & Barto, p. 96) and fixed start points. Any difference?\n",
    "\n",
    "Take a look at the value function you get when you run the algorithm multiple times (with fixed # of iterations). Any observations?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  0.   -14.89 -20.72 -22.48]\n",
      " [-14.33 -18.32 -20.87 -21.31]\n",
      " [-20.25 -20.02 -18.3  -14.76]\n",
      " [-23.01 -20.63 -14.43   0.  ]]\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 3*: Monte Carlo action value function estimation\n",
    "\n",
    "Use the same idea as in exercise 2 to estimate q function.\n",
    "\n",
    "*) - not mandatory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 4*: Monte Carlo control\n",
    "\n",
    "Compute the optimal policy for the 4x4 gridworld example. Start with random policy. Consider the epsilon adjustment schedule - can it in practise be 1/k, or is something more conservative better? Can you think of any other tricks to manage the noisiness of MC?\n",
    "\n",
    "*) - not mandatory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
