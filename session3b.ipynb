{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TX00DQ05-3001 Exercises 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import numpy.linalg as LA\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 1: Sample behaviour of an MDP\n",
    "\n",
    "Let's take (again) a look at Sutton & Barto example 4.1 gridworld. On each iteration start at every (non-terminating) state and sample actions in succeeding states by selecting them from uniform distribution (each action - up, down, left, right - is equally probable). Run the episode until terminal state is encountered. Collect statistics to calculate average number of steps needed before completion for each start state. Should this number match with something you have seen earlier in the exercises?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 0), (3, 3)]\n",
      "[[0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]]\n",
      "14\n",
      "[[0.   0.11 0.29 0.3 ]\n",
      " [0.13 0.18 0.17 0.21]\n",
      " [0.28 0.27 0.13 0.14]\n",
      " [0.25 0.24 0.15 0.  ]]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def calculateValueForCurrentState(value,stepcost):\n",
    "    #print(\"(\",stepcost,\"+\",value,\")*0.25 = \",((stepcost+(value))*0.25))\n",
    "    return ((stepcost+(value))*0.25)\n",
    "\n",
    "\n",
    "def checkWithBoundary(max,r,c,oR,oC):\n",
    "    #print(\"-nstate: \",r,\",\",c)\n",
    "    #print(V)\n",
    "    if 0 > r or max <= r or 0 > c or max <= c :\n",
    "        #print(oR,\",\",oC,\" value:\",V[oR,oC])\n",
    "        return (oR,oC)\n",
    "    else:\n",
    "        #print(r,\",\",c,\" value:\",V[r,c])\n",
    "        return (r,c)\n",
    "\n",
    "    \n",
    "    \n",
    "def Actions(val):\n",
    "    if val == 0:\n",
    "        return [1,0]\n",
    "    elif val == 1:\n",
    "        return [-1,0]\n",
    "    elif val == 2:\n",
    "        return [0,1]\n",
    "    elif val == 3:\n",
    "        return [0,-1]\n",
    "    \n",
    "\n",
    "def randomAction():\n",
    "    return Actions(random.randint(0,3))\n",
    "\n",
    "def makeAction(maxsize,r,c):\n",
    "    a = randomAction()\n",
    "    \n",
    "    \n",
    "    return checkWithBoundary(maxsize,r+a[0],c+a[1],r,c)\n",
    "\n",
    "def checkIfInTerminatingPoint(pos,term):\n",
    "    for t in term:\n",
    "        if pos == t:\n",
    "            return True\n",
    "    return False\n",
    "    \n",
    "class Agent(object):\n",
    "    steps = 0\n",
    "    def __init__(self, startPos):\n",
    "        self.startPos = startPos\n",
    "        self.pos = startPos\n",
    "    \n",
    "def MC_pathlengths(maxiters,gridworld,terminating):\n",
    "    \n",
    "    lengths= gridworld\n",
    "    agents =[]\n",
    "    \n",
    "    for x in range(lengths.shape[0]):\n",
    "        for y in range(lengths.shape[1]):\n",
    "            if not checkIfInTerminatingPoint((x,y),terminating):\n",
    "                agents.append( Agent((x,y)) )\n",
    "    print(len(agents))\n",
    "    for i in range(1, maxiters):\n",
    "\n",
    "        goals = []\n",
    "        calc = 0\n",
    "        \n",
    "        while (len(agents)>0):\n",
    "            for agent in agents:\n",
    "                agent.pos = makeAction(lengths.shape[0],agent.pos[0],agent.pos[1])\n",
    "                agent.steps += 1\n",
    "                if (checkIfInTerminatingPoint(agent.pos,terminating)):\n",
    "                    agents.remove(agent)\n",
    "                    goals.append(agent)\n",
    "                    break\n",
    "                    \n",
    "            \n",
    "        for agent in goals:\n",
    "            mean = ((i-1)/i) * lengths[agent.startPos] + agent.steps/i\n",
    "            #print(((i-1)/i),\"*\",lengths[agent.startPos],\"+\",agent.steps/i,\"=\",mean)\n",
    "            lengths[agent.startPos] = mean\n",
    "            agent.steps = 0\n",
    "            agent.pos = agent.startPos\n",
    "            \n",
    "        agents = goals\n",
    "    return lengths       \n",
    "    \n",
    "    \n",
    "def CreateGridWorld(gridsize,terminating = None,numberOfTerminating = None):\n",
    "    \n",
    "\n",
    "    if(terminating == None):\n",
    "        if(numberOfTerminating == 0 or numberOfTerminating == None):\n",
    "            numberOfTerminating = 1\n",
    "        terminating = []\n",
    "        for n in range(numberOfTerminating):\n",
    "            terminating.append((random.randint(0,gridsize[0]-1), \\\n",
    "                        random.randint(0,gridsize[1]-1)))\n",
    "        \n",
    "        \n",
    "    gridworld = np.zeros((gridsize[0], gridsize[1]))\n",
    "    \n",
    "    print(terminating)\n",
    "    print(gridworld)\n",
    "    return gridworld, terminating\n",
    "\n",
    "\n",
    "gridworld,terminating = CreateGridWorld((4,4),[(0,0), (3,3)],)\n",
    "with np.printoptions(precision=2):\n",
    "    print(MC_pathlengths(1000,gridworld,terminating))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2: Monte Carlo state value function estimation. \n",
    "\n",
    "Calculate state-value function V for the gridworld of Sutton & Barto example 4.1 using first-visit or every-visit Monte Carlo policy evaluation (see for example page 92 of Sutton & Barto). Policy to be evaluated is the same as before; each action (up, down, left, right) is equally probable.  Action that would result in leaving the grid (for example moving up in top row) will leave state unchanged (but action has been taken). Gamma (discount factor) is assumed to be = 1, ie. no discounting.\n",
    "\n",
    "Try out both exploring starts (see Sutton & Barto, p. 96) and fixed start points. Any difference?\n",
    "\n",
    "Take a look at the value function you get when you run the algorithm multiple times (with fixed # of iterations). Any observations?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 0), (3, 3)]\n",
      "[[0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]]\n",
      "14\n",
      "[[  0.   -13.97 -18.04 -21.17]\n",
      " [-14.7  -17.93 -18.43 -20.03]\n",
      " [-21.08 -19.73 -17.05 -13.61]\n",
      " [-23.94 -19.62 -13.19   0.  ]]\n"
     ]
    }
   ],
   "source": [
    "def MC_randomwalk(maxiters,gridworld,terminating):\n",
    "    \n",
    "    lengths= gridworld\n",
    "    agents =[]\n",
    "    \n",
    "    for x in range(lengths.shape[0]):\n",
    "        for y in range(lengths.shape[1]):\n",
    "            if not checkIfInTerminatingPoint((x,y),terminating):\n",
    "                agents.append( Agent((x,y)) )\n",
    "    print(len(agents))\n",
    "    for i in range(1, maxiters):\n",
    "\n",
    "        goals = []\n",
    "        calc = 0\n",
    "        \n",
    "        \n",
    "        for agent in agents:\n",
    "            steps = []\n",
    "            steps.append(agent.pos)\n",
    "            while(True):\n",
    "                agent.pos = makeAction(lengths.shape[0],agent.pos[0],agent.pos[1])\n",
    "                \n",
    "                agent.steps += 1\n",
    "                if (checkIfInTerminatingPoint(agent.pos,terminating)):\n",
    "                    goals.append(agent)\n",
    "                    break\n",
    "                steps.append(agent.pos)    \n",
    "            num =1\n",
    "            agent.pos = agent.startPos\n",
    "            steps.reverse()\n",
    "            for step in steps:\n",
    "                \n",
    "                mean= (lengths[step]-1/num)\n",
    "                #print(lengths[step],\"- 1 /\",num,\" = \",mean)\n",
    "                \n",
    "                lengths[step]=mean\n",
    "                \n",
    "        \n",
    "    \n",
    "    return lengths/100\n",
    "\n",
    "gridworld,terminating = CreateGridWorld((4,4),[(0,0), (3,3)],)\n",
    "with np.printoptions(precision=2):\n",
    "    print(MC_randomwalk(100,gridworld,terminating))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 3*: Monte Carlo action value function estimation\n",
    "\n",
    "Use the same idea as in exercise 2 to estimate q function.\n",
    "\n",
    "*) - not mandatory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 4*: Monte Carlo control\n",
    "\n",
    "Compute the optimal policy for the 4x4 gridworld example. Start with random policy. Consider the epsilon adjustment schedule - can it in practise be 1/k, or is something more conservative better? Can you think of any other tricks to manage the noisiness of MC?\n",
    "\n",
    "*) - not mandatory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
